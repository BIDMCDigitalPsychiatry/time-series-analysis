{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196a326a-0339-4d92-9b72-09af172cfb6b",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac272be-ea41-4848-86fc-51ed44deecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "from datetime import timedelta\n",
    "tf.random.set_seed(\n",
    "1234\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18821f41-bc9d-4e1e-baf4-66d6d796d13b",
   "metadata": {},
   "source": [
    "## Configure Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747058a-9e7a-4448-933e-28ed06776a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing configuration specifies the number of previous time steps (WINDOW_SIZE) \n",
    "# and future time steps (HORIZON) to consider for each data sample.\n",
    "WINDOW_SIZE = 5\n",
    "HORIZON = 1\n",
    "\n",
    "# Define the target variable for prediction and a list of feature column names that the model will use as inputs. \n",
    "# These columns represent various metrics that could potentially influence the target variable.\n",
    "TARGET_COLUMN = 'panss_total'\n",
    "FEATURE_COLUMNS = ['Mood', 'Sleep', 'Psychosis', 'Social', 'Anxiety',\n",
    "                   'Hometime', 'Entropy', 'Screen_Duration',\n",
    "                   'PHQ9', 'GAD', 'sfs_composite', 'PSQI']\n",
    "# Combine the feature columns and the target column into a single list that will be used for data processing.\n",
    "ALL_COLUMNS = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "\n",
    "# Establish the fractions of the dataset to be allocated to the training, validation, and test sets. \n",
    "train_frac = 0.70\n",
    "validation_frac = 0.15\n",
    "test_frac = 0.15\n",
    "\n",
    "# Additionally, assert that the sum of the proportions equals 1, or 100% of the data, which ensures that no data is unaccounted for or duplicated across sets.\n",
    "assert train_frac + validation_frac + test_frac == 1\n",
    "\n",
    "# Set a variability threshold and minimum column count to filter out sequences without significant change.\n",
    "# Due to high missingness in the dataset, forward fill imputation could skew the learning process due to a lack of meaningful variation.\n",
    "VARIABILITY_THRESHOLD = 2\n",
    "VARIABILITY_COLUMN_COUNT = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf03006-aa41-41b5-8bf2-d4e70c6bc27e",
   "metadata": {},
   "source": [
    "## Load File into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda6852-16ba-4434-a81e-791fd37aeafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_df = pd.read_csv('forwardfill.csv')\n",
    "impute_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a10c3-b8ee-4b6d-9669-71ba4feb9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns relevant to our analysis and filtering the dataset.\n",
    "# Among the columns, we are including participant identifiers, site information, and date values in addition to:\n",
    "# Active Features:\n",
    "# - EMA Surveys: 'Mood', 'Social', 'Sleep', 'Psychosis', 'Anxiety'\n",
    "# - Structured Surveys: 'PHQ9', 'GAD', 'PSQI'\n",
    "# Passive Features:\n",
    "# - Smartphone Generated: 'Hometime', 'Entropy', 'Screen_Duration'\n",
    "# Target Scores + Subscores\n",
    "# - PANSS: 'panss_total', 'panss_neg', 'panss_pos', 'panss_gen'\n",
    "# - SFS: 'sfs_scaled', 'social_engagement', 'interpersonal_behavior', 'prosocial_activities', 'recreation', 'competence', 'performance', 'employment'\n",
    "df = impute_df[['Date', 'IID', 'site',\n",
    "                 'Mood', 'Social', 'Sleep', 'Psychosis', 'Anxiety',\n",
    "                 'Hometime', 'Entropy', 'Screen_Duration', \n",
    "                 'PHQ9', 'GAD', 'PSQI', \n",
    "                 'sfs_scaled', 'social_engagement', 'interpersonal_behavior', 'prosocial_activities', 'recreation', 'competence', 'performance', 'employment',\n",
    "                 'panss_total', 'panss_neg', 'panss_pos', 'panss_gen']]\n",
    "\n",
    "# After selecting our columns, we drop any rows that contain null values which is vital for the accuracy of the LSTM model.\n",
    "df = df.dropna()\n",
    "# Additionally, we standardize the 'Date' column to a consistent format to avoid parsing issues later on in the analysis.\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "len(df['IID'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa0598-3938-45a7-bf75-093d0e23fec0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "48ca0204-d0f7-44ef-8a7a-c92c2e08c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train: 5157\n",
      " X_validation: 649\n",
      " X_test: 433\n",
      " y_train: 5157\n",
      " y_validation: 649\n",
      " y_test: 433\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MinMaxScaler to normalize feature values between 0 and 1\n",
    "X_scaler = MinMaxScaler()\n",
    "Y_scaler = MinMaxScaler()\n",
    "\n",
    "# Define a function to partition time-series data for a participant into training, validation, and testing sets\n",
    "def split_time_series_data(participant_data, train_frac, validation_frac):\n",
    "    # Ensure the data is sorted chronologically to maintain the sequence integrity.\n",
    "    participant_data_sorted = participant_data.sort_values('Date')\n",
    "\n",
    "    # Determine the split indices based on the specified fractions.\n",
    "    total_records = len(participant_data_sorted)\n",
    "    train_end = int(train_frac * total_records)\n",
    "    validation_end = train_end + int(validation_frac * total_records)\n",
    "\n",
    "    # Divide the dataset into the respective sets.\n",
    "    train_data = participant_data_sorted.iloc[:train_end]\n",
    "    validation_data = participant_data_sorted.iloc[train_end:validation_end]\n",
    "    test_data = participant_data_sorted.iloc[validation_end:]\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Define a function to create input-output sequence pairs for LSTM training, based on variability in the data\n",
    "def prepare_sequences(data):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - WINDOW_SIZE - HORIZON):\n",
    "        window = data.iloc[i:(i + WINDOW_SIZE + HORIZON)][ALL_COLUMNS]\n",
    "        target_values = window[TARGET_COLUMN]\n",
    "\n",
    "        # Analyze the standard deviation to identify columns with significant variability.\n",
    "        columns_meeting_threshold = np.sum(window.std() > VARIABILITY_THRESHOLD)\n",
    "\n",
    "        # Include a sequence if the target variable changes or if sufficient variability in column values is observed.\n",
    "        if (target_values.nunique() > 1) or (columns_meeting_threshold >= VARIABILITY_COLUMN_COUNT):\n",
    "            X.append(window.iloc[:-HORIZON - 1][FEATURE_COLUMNS].values)\n",
    "            y.append(window.iloc[-1][TARGET_COLUMN])\n",
    "    return X, y\n",
    "\n",
    "# Create empty lists to hold the data splits for all participants.\n",
    "X_train, X_validation, X_test = [], [], []\n",
    "y_train, y_validation, y_test = [], [], []\n",
    "\n",
    "# Obtain a list of unique participant identifiers.\n",
    "participants = df['IID'].unique() \n",
    "\n",
    "# Process the data for each participant individually.\n",
    "for participant in participants:\n",
    "    # Select the data for the current participant.\n",
    "    participant_data = df[df['IID'] == participant]\n",
    "\n",
    "    # Apply the data splitting function to obtain training, validation, and testing sets.\n",
    "    train_data, validation_data, test_data = split_time_series_data(participant_data, train_frac, validation_frac)\n",
    "\n",
    "    # Generate sequences for each partition of the data.\n",
    "    X_tr, y_tr = prepare_sequences(train_data)\n",
    "    X_val, y_val = prepare_sequences(validation_data)\n",
    "    X_te, y_te = prepare_sequences(test_data)\n",
    "\n",
    "    # Append the generated sequences to the corresponding aggregate lists.\n",
    "    X_train.extend(X_tr)\n",
    "    y_train.extend(y_tr)\n",
    "    X_validation.extend(X_val)\n",
    "    y_validation.extend(y_val)\n",
    "    X_test.extend(X_te)\n",
    "    y_test.extend(y_te)\n",
    "\n",
    "\n",
    "# Convert the sequence lists into numpy arrays for compatibility with machine learning models.\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_validation = np.array(X_validation)\n",
    "y_validation = np.array(y_validation)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)    \n",
    "\n",
    "# Instantiate a MinMaxScaler to normalize the features.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Fit the scaler on the training data, ensuring all data is stacked to match the scaler's expected input format.\n",
    "scaler.fit(np.vstack(X_train))\n",
    "\n",
    "# Apply the fitted scaler to normalize the training, validation, and test data.\n",
    "X_train_scaled = scaler.transform(np.vstack(X_train)).reshape(X_train.shape)\n",
    "X_validation_scaled = scaler.transform(np.vstack(X_validation)).reshape(X_validation.shape)\n",
    "X_test_scaled = scaler.transform(np.vstack(X_test)).reshape(X_test.shape)\n",
    "\n",
    "# Fit and transform the target variable using the scaler, and then flatten the resulting array.\n",
    "y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_validation_scaled = scaler.transform(y_validation.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler.transform(y_test.reshape(-1, 1)).flatten() \n",
    "\n",
    "# Output basic metrics regarding the size of each data split for review.\n",
    "print(f\" X_train: \" + str(len(X_train)))\n",
    "print(f\" X_validation: \" + str(len(X_validation)))\n",
    "print(f\" X_test: \" + str(len(X_test)))\n",
    "\n",
    "print(f\" y_train: \" + str(len(y_train)))\n",
    "print(f\" y_validation: \" + str(len(y_validation)))\n",
    "print(f\" y_test: \" + str(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41395c3-2bcd-41f9-843d-bdba6635e089",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3ea73-b2f1-45ea-91ef-67067214a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting batch size and buffer size for data shuffling and batching\n",
    "batch_size = 110\n",
    "buffer_size = 2048\n",
    "\n",
    "# Preparing the training data\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train_scaled))\n",
    "# Caching the dataset for performance, shuffling it for randomness, and batching for training\n",
    "train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "# Preparing the validation data in a similar way to the training data\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_validation_scaled, y_validation_scaled))\n",
    "val_data = val_data.cache().shuffle(buffer_size).batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb3ad2-40fa-4fb3-8323-d727b3f3499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simple RNN model for baseline comparison\n",
    "rnn_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(4, input_shape=X_train.shape[-2:]), # Input layer with SimpleRNN\n",
    "    tf.keras.layers.Dense(20, activation='relu'), # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(1) # Output layer with linear activation for regression\n",
    "])\n",
    "# Compiling the RNN model with mean squared error loss function and Adam optimizer\n",
    "rnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Displaying the model architecture for review\n",
    "rnn_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4af278-388a-4a79-a007-6638ae2ea1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a more complex LSTM model to potentially capture more complex patterns\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Bidirectional LSTM layer to process input in both directions\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, return_sequences=True),\n",
    "                                 input_shape=X_train.shape[-2:]),\n",
    "    tf.keras.layers.Dense(20, activation='relu'), # Dense layer with ReLU activation\n",
    "    # Second Bidirectional LSTM layer to further process the sequence data\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dense(20, activation='relu'), # Additional dense layers with ReLU activation\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25), # Dropout for regularization to prevent overfitting\n",
    "    tf.keras.layers.Dense(units=1), # Output layer with linear activation for regression\n",
    "])\n",
    "# Compiling the LSTM model with mean squared error loss function and Adam optimizer\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# Displaying the LSTM model architecture for review\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072f419-ec4b-4e08-8cc1-b7ce9c421f18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1f09f-3ce9-4cc1-b522-2bacfb3d8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the best model will be saved\n",
    "model_path = 'PANSS_Total_LSTM_wind5_horz1.h5'  \n",
    "\n",
    "# Set up an EarlyStopping callback to prevent overfitting. Training will stop if 'val_loss' doesn't improve for 15 epochs.\n",
    "early_stopings = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0, \n",
    "    patience=15, \n",
    "    verbose=1, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Set up a ModelCheckpoint callback to save the model with the lowest 'val_loss' after each epoch.\n",
    "checkpoint =  tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_path, \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min', \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Combine the callbacks into a list to be used in the model fitting process.\n",
    "callbacks = [early_stopings, checkpoint]\n",
    "\n",
    "# Fit the LSTM model to the training data, using the validation data for evaluation and the defined callbacks for early stopping and checkpointing.\n",
    "history = lstm_model.fit(\n",
    "    train_data, \n",
    "    epochs=150, \n",
    "    steps_per_epoch=100, \n",
    "    validation_data=val_data, \n",
    "    validation_steps=100, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d1f9f-5990-49b6-a3a6-635b6c35e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss over epochs\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddc2a7-78e2-4113-957e-103feafee169",
   "metadata": {},
   "source": [
    "## Assessing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45501960-8f0e-42b0-b406-bdb75c3d37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "# Initialize a KFold object with shuffle to ensure randomness and a fixed random state for reproducibility\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results of metrics for each fold\n",
    "rmse_values = []\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "\n",
    "# The following cross-validation is performed on the test set and considered as the final evaluation \n",
    "# step since the model parameters were not tuned based on this data. This evaluation method is separate \n",
    "# from the typical cross-validation approach which often partitions the training set.\n",
    "\n",
    "# Iterate through each fold, splitting the data into training and testing sets for final evaluation\n",
    "for train_index, test_index in kf.split(X_test_scaled):\n",
    "    # Use the indices to partition the scaled test set\n",
    "    X_train_fold, X_test_fold = X_test_scaled[train_index], X_test_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_test_scaled[train_index], y_test_scaled[test_index]\n",
    "\n",
    "    # Load your pre-trained model (the path should point to the model file)\n",
    "    model = tf.keras.models.load_model('PANSS_Total_LSTM_wind5_horz1.h5')\n",
    "\n",
    "    # Use the model to make predictions on the test fold\n",
    "    pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "    # Inverse transform the predictions to original scale using the scaler fitted on the training data\n",
    "    pred_fold_inverse = scaler.inverse_transform(pred_fold)\n",
    "\n",
    "    # Reshape y_test_fold to 2D array to match the shape expected by the scaler\n",
    "    y_test_fold_2d = y_test_fold.reshape(-1, 1)\n",
    "\n",
    "    # Inverse transform the actual test fold values to original scale\n",
    "    actual_fold_inverse = scaler.inverse_transform(y_test_fold_2d)\n",
    "\n",
    "    # Calculate Root Mean Squared Error (RMSE) for the current fold\n",
    "    rmse_fold = np.sqrt(np.mean(np.square(np.subtract(actual_fold_inverse, pred_fold_inverse))))\n",
    "    # Calculate Mean Absolute Error (MAE) for the current fold\n",
    "    mae_fold = np.mean(np.abs(np.subtract(actual_fold_inverse, pred_fold_inverse)))\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE) for the current fold\n",
    "    #    np.maximum utilized in the denominator to avoid division by 0 errors\n",
    "    mape_fold = np.mean(np.abs(np.divide(np.subtract(actual_fold_inverse, pred_fold_inverse), \n",
    "                                         np.maximum(np.abs(actual_fold_inverse), 1)))) * 100\n",
    "\n",
    "    # Append the calculated metrics to their respective lists\n",
    "    rmse_values.append(rmse_fold)\n",
    "    mae_values.append(mae_fold)\n",
    "    mape_values.append(mape_fold)\n",
    "\n",
    "# Calculate the mean and standard deviation of metrics across all folds\n",
    "average_rmse = np.mean(rmse_values)\n",
    "average_mae = np.mean(mae_values)\n",
    "average_mape = np.mean(mape_values)\n",
    "std_rmse = np.std(rmse_values)\n",
    "std_mae = np.std(mae_values)\n",
    "std_mape = np.std(mape_values)\n",
    "\n",
    "# Print the results to summarize the cross-validation performance\n",
    "print(f\"Average RMSE across {num_folds} folds: {average_rmse}\")\n",
    "print(f\"Average MAE across {num_folds} folds: {average_mae}\")\n",
    "print(f\"Average MAPE across {num_folds} folds: {average_mape}\")\n",
    "print(f\"Standard Deviation of RMSE across {num_folds} folds: {std_rmse}\")\n",
    "print(f\"Standard Deviation of MAE across {num_folds} folds: {std_mae}\")\n",
    "print(f\"Standard Deviation of MAPE across {num_folds} folds: {std_mape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
